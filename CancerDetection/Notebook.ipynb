{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Load in dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T03:20:00.218545Z","iopub.status.busy":"2023-11-07T03:20:00.218110Z","iopub.status.idle":"2023-11-07T03:20:01.544475Z","shell.execute_reply":"2023-11-07T03:20:01.543487Z","shell.execute_reply.started":"2023-11-07T03:20:00.218509Z"},"trusted":true},"outputs":[],"source":["# Libraries\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T03:20:01.547288Z","iopub.status.busy":"2023-11-07T03:20:01.546761Z","iopub.status.idle":"2023-11-07T03:20:01.622054Z","shell.execute_reply":"2023-11-07T03:20:01.620543Z","shell.execute_reply.started":"2023-11-07T03:20:01.547255Z"},"trusted":true},"outputs":[],"source":["# Load data and get a quick look\n","fp = \"data.csv\"\n","df = pd.read_csv(fp)\n","display(df.head(10))"]},{"cell_type":"markdown","metadata":{},"source":["# Data Cleaning\n","Here I will check for any missing values, duplications, etc..."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T03:20:01.623841Z","iopub.status.busy":"2023-11-07T03:20:01.623474Z","iopub.status.idle":"2023-11-07T03:20:01.643797Z","shell.execute_reply":"2023-11-07T03:20:01.642770Z","shell.execute_reply.started":"2023-11-07T03:20:01.623807Z"},"trusted":true},"outputs":[],"source":["nData = df.shape[0]\n","# Check for any missing values\n","dictionary = {'NumMissingValues':df.isna().sum(),'RatioMissingValues':df.isna().sum()/nData}\n","# display(df.isna().sum())\n","# display(df.isna().sum()/nData)\n","display(pd.DataFrame(dictionary))"]},{"cell_type":"markdown","metadata":{},"source":["So no missing values apart from a column `Unnamed: 32` which are all NaN. Meaning they can be dropped"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T03:20:01.646107Z","iopub.status.busy":"2023-11-07T03:20:01.645641Z","iopub.status.idle":"2023-11-07T03:20:01.662277Z","shell.execute_reply":"2023-11-07T03:20:01.661344Z","shell.execute_reply.started":"2023-11-07T03:20:01.646023Z"},"trusted":true},"outputs":[],"source":["# Drop the unnamed:32 column\n","df = df.drop(labels=\"Unnamed: 32\", axis=1)\n","#Print to confirm it is removed\n","df.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T03:20:01.664924Z","iopub.status.busy":"2023-11-07T03:20:01.664572Z","iopub.status.idle":"2023-11-07T03:20:01.676221Z","shell.execute_reply":"2023-11-07T03:20:01.674975Z","shell.execute_reply.started":"2023-11-07T03:20:01.664893Z"},"trusted":true},"outputs":[],"source":["#Any duplicates?\n","nVals = df.shape[0]\n","nUniqueId = len(df['id'].unique())\n","print(nUniqueId, nVals)"]},{"cell_type":"markdown","metadata":{},"source":["No duplicated `id` values so unlikely for the data to be duplicated in its entirety"]},{"cell_type":"markdown","metadata":{},"source":["# Exploratory Data Analysis\n","The plan is to...\n","1. Look for class imbalance, which will help determine which scoring metric to use to evaluate the data.\n","\n","2. Look at the different distributions and see if...\n","    1. Normalisations or other transformations are necessary?\n","    2. Are there any anomalous values which I will need to impute?\n","    3. Even within a single class metric are there any clear differences between the two sets of data.\n","    \n","3. For visualisation purposes will dimension reduction help shed any light, produce any clear differences? I will assess unsupervised and supervised dimension reduction techniques.\n","    1. For unsupervised reduction: First I will try PCA and if that doesn't work I will try UMAP\n","        1. **PCA**: PCA is unbiassed and tries to reduce the variance into new putative dimensions known as principal components. If successful these components can even be used as features.\n","        2. **UMAP**:If PCA doesn't work effectively, a final strategy is to use UMAP which uses local neighbours to create a manifold in the specified dimenision. NB this manifold is not linear or scalable. So it is not advised to utilise this method if performing subsequent clustering analysis.\n","        \n","    2. For supervised techniques: I will consider two methods Partial Least Squares (PLS), and also Fisher's Discriminant Analysis.\n","       1. **PLS**: Without going into a lot of details PLS utilises matrix multiplication to multiply the input matrix with a putative loading matrix (n_features by n_components) to produce values as similar as possible to the target variable. Essentially think of it a bit like linear regression with n components where each component has weighted contributions from all of our existing variables.\n","       2. **Fischers Discriminant Analysis**: Here this essentially attempts to reduce everything down to the minimum dimensions needed (e.g. if 2 classes, then one dimension, 4 classes = 3D, etc...) by working out the maximum distance of the centroids of the classes."]},{"cell_type":"markdown","metadata":{},"source":["## Is there a class imbalance?"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T03:20:01.680133Z","iopub.status.busy":"2023-11-07T03:20:01.679534Z","iopub.status.idle":"2023-11-07T03:20:01.913991Z","shell.execute_reply":"2023-11-07T03:20:01.912750Z","shell.execute_reply.started":"2023-11-07T03:20:01.680094Z"},"trusted":true},"outputs":[],"source":["# Does the data set have a good split of malign and benign tumors?\n","ax = sns.countplot(data=df, x='diagnosis')\n","ax.bar_label(ax.containers[0])"]},{"cell_type":"markdown","metadata":{},"source":["There are approximately 150 more benign tumors. Which will mean I should use the F1 score when measuring accuracy as it is based on the harmonic mean."]},{"cell_type":"markdown","metadata":{},"source":["## Looking at spreads and distributions of individual features/variables"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T03:20:01.915964Z","iopub.status.busy":"2023-11-07T03:20:01.915561Z","iopub.status.idle":"2023-11-07T03:20:14.520109Z","shell.execute_reply":"2023-11-07T03:20:14.519156Z","shell.execute_reply.started":"2023-11-07T03:20:01.915930Z"},"trusted":true},"outputs":[],"source":["nCols = len(df.columns)\n","nRow = 8\n","nCol = 4\n","fig, ax = plt.subplots(nRow,nCol, figsize=(15,25))\n","ColNames = df.columns\n","\n","\n","for r in range(nRow):\n","    for c in range(nCol):\n","        i = (r*nCol) + c\n","        if i < nCols:\n","            col = ColNames[i]\n","            if i != nCols-1:\n","                sns.histplot(data=df, x=col, hue='diagnosis', kde=True, ax=ax[r,c], legend=False)\n","            else:\n","                sns.histplot(data=df, x=col, hue='diagnosis', kde=True, ax=ax[r,c], legend=True)"]},{"cell_type":"markdown","metadata":{},"source":["So we can begin to see some clear deliniations between the two groups. For example malign cells will be bigger meaning the values for `perimeter_worst`, `area_worst` and other size metrics show a visibly different distribution. Other features such as `fractal_dimension_mean` don't appear to have a clear difference.\n","\n","As an additional point a lot of the data appears skewed. Which suggests that the data may need to be transformed. As a first pass I suggest min/max normalisation as this will be useful when evaluating models such as SVM and logistic regression as the values of each feature will be weighted equally. NB for tree based models such as light GBM, random forest etc... normalisation is not needed as only one feature is considered at a time.\n","\n","Additionally there do not appear to be any extremely anomalous results `perimeter_se` and `concavity_se` are close to having extreme values but they may just be the results of a long tail so I will not discard them."]},{"cell_type":"markdown","metadata":{},"source":["## Normalisation\n","I will implement a MinMaxScaler for all of these values so that they will be normalised"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T03:20:14.522090Z","iopub.status.busy":"2023-11-07T03:20:14.521585Z","iopub.status.idle":"2023-11-07T03:20:14.642565Z","shell.execute_reply":"2023-11-07T03:20:14.641199Z","shell.execute_reply.started":"2023-11-07T03:20:14.522055Z"},"trusted":true},"outputs":[],"source":["# Normalise the dataframe\n","from sklearn.preprocessing import MinMaxScaler\n","\n","df_num = df.drop(labels=['id','diagnosis'], axis=1)\n","mms = MinMaxScaler()\n","mms.fit(df_num)\n","data_norm = mms.transform(df_num)\n","df_norm = pd.DataFrame(data_norm, columns=mms.get_feature_names_out())\n","df_norm = pd.concat([df[['id','diagnosis']],df_norm], axis=1)\n","display(df_norm)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T03:20:14.646368Z","iopub.status.busy":"2023-11-07T03:20:14.645877Z","iopub.status.idle":"2023-11-07T03:20:28.092589Z","shell.execute_reply":"2023-11-07T03:20:28.091173Z","shell.execute_reply.started":"2023-11-07T03:20:14.646333Z"},"trusted":true},"outputs":[],"source":["nCols = len(df.columns)\n","nRow = 8\n","nCol = 4\n","fig, ax = plt.subplots(nRow,nCol, figsize=(15,25))\n","ColNames = df_norm.columns\n","\n","\n","for r in range(nRow):\n","    for c in range(nCol):\n","        i = (r*nCol) + c\n","        if i < nCols:\n","            col = ColNames[i]\n","            if i != nCols-1:\n","                sns.histplot(data=df_norm, x=col, hue='diagnosis', kde=True, ax=ax[r,c], legend=False)\n","            else:\n","                sns.histplot(data=df_norm, x=col, hue='diagnosis', kde=True, ax=ax[r,c], legend=True)"]},{"cell_type":"markdown","metadata":{},"source":["The MinMaxScaler has helped the data appear far more normal, and in many cases it has also helped segregate the two classes."]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2023-11-02T05:34:15.923613Z","iopub.status.busy":"2023-11-02T05:34:15.923211Z","iopub.status.idle":"2023-11-02T05:34:15.931423Z","shell.execute_reply":"2023-11-02T05:34:15.930178Z","shell.execute_reply.started":"2023-11-02T05:34:15.923582Z"}},"source":["## Dimension reduction\n","This is just for a basic investigation to see how well a combination of values are at splitting the data even more. This will give an indication as to whether we can speed up the calculations even more by focussing on particular features\n","\n","#### Unsupervised dimension reductions\n","1) PCA\n","2) UMAP"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T03:20:28.094506Z","iopub.status.busy":"2023-11-07T03:20:28.094171Z","iopub.status.idle":"2023-11-07T03:20:28.874930Z","shell.execute_reply":"2023-11-07T03:20:28.874068Z","shell.execute_reply.started":"2023-11-07T03:20:28.094477Z"},"trusted":true},"outputs":[],"source":["# PCA\n","from sklearn.decomposition import PCA\n","data = df_norm.drop(labels=['id','diagnosis'], axis=1)\n","maxD = data.shape[1]\n","\n","pca = PCA(n_components=maxD)\n","pca.fit(data)\n","VarExp = pca.explained_variance_ratio_\n","nDim = np.arange(1,maxD+1)\n","\n","PCA_df = pd.DataFrame({'PC':np.arange(1,maxD+1),'VarianceExplained':VarExp})\n","PCA_df['CumulativeVarExp'] = PCA_df['VarianceExplained'].cumsum()\n","# Now plot\n","fig, ax = plt.subplots(1,2, figsize=(10,5))\n","ax[0].plot(nDim, VarExp)\n","ax[0].set_xlabel('PC #')\n","ax[0].set_ylabel('Variance explained(ratio)')\n","ax[0].set_title('Variance explained by each PC')\n","\n","ax[1].plot(nDim, PCA_df['CumulativeVarExp'])\n","ax[1].set_xlabel('PC #')\n","ax[1].set_ylabel('Cumulative Variance explained(ratio)')\n","ax[1].set_title('Cumulative Variance explained by each PC')\n","plt.title('Exploration of PCA contributions')\n","plt.show()\n","display(PCA_df)"]},{"cell_type":"markdown","metadata":{},"source":["There is a steep slope after 3 dimensions, but 2 dimensions brings out approximately 70% of the variance already meaning it should already be very informative. Lets plot it and have a look."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T03:20:28.877102Z","iopub.status.busy":"2023-11-07T03:20:28.876478Z","iopub.status.idle":"2023-11-07T03:20:29.335268Z","shell.execute_reply":"2023-11-07T03:20:29.334207Z","shell.execute_reply.started":"2023-11-07T03:20:28.877064Z"},"trusted":true},"outputs":[],"source":["nDim = 2\n","pca = PCA(n_components=nDim)\n","Xt = pca.fit_transform(data)\n","df_Xt = pd.DataFrame(Xt, columns=['PC1','PC2'])\n","df_Xt = pd.concat([df['diagnosis'],df_Xt], axis=1)\n","sns.scatterplot(data=df_Xt, x='PC1', y='PC2', hue='diagnosis')\n","plt.xlabel('PC1')\n","plt.ylabel('PC2')\n","plt.title('PCA on Breast Cancer image data set')"]},{"cell_type":"markdown","metadata":{},"source":["This is really nice. The data is naturally segregated, with a small amount of error on an overlapping section around (0,0). This means an ML solution should be viable. NB as this is already nice, I'm not going to waste time performing a UMAP. \n","\n","Furthermore,for future feature engineering investigations I will use the first 10 PCs as they explain 95% of the variance which should be more than sufficient. \n","\n","## Supervised dimension reduction\n","An alternative way, which can also lead to predictions is to perform supervised dimension reduction. \n","\n","### Partial least squares\n","My first method of choise is Partial Least Squares (PLS), which uses the target variable/feature, to create new features. This means I will need to re-code the target variable (diagnosis) into 0-benign and 1-malignant"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T03:20:29.337086Z","iopub.status.busy":"2023-11-07T03:20:29.336683Z","iopub.status.idle":"2023-11-07T03:20:56.897568Z","shell.execute_reply":"2023-11-07T03:20:56.896471Z","shell.execute_reply.started":"2023-11-07T03:20:29.337057Z"},"trusted":true},"outputs":[],"source":["from sklearn.cross_decomposition import PLSRegression\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import mean_squared_error\n","df_pls = df_norm.copy()\n","# Re-code diagnosis\n","df_pls['diagnosis'] = df_pls['diagnosis'].replace(['M','B'],[1,0])\n","\n","MaxFolds = 10\n","\n","# Evaluate the accuracy of the regressor\n","temp_dict = {\n","    'nFolds': [],\n","    'nDim': [],\n","    'rmse': []\n","}\n","for f in range(2,MaxFolds+1):\n","    kf = KFold(n_splits=f, shuffle=True, random_state=42)\n","    for train, test in kf.split(df_pls):\n","        X_train = df_pls.iloc[train].drop(['id','diagnosis'], axis=1)\n","        X_test = df_pls.iloc[test].drop(['id','diagnosis'], axis=1)\n","        y_train = df_pls.iloc[train]['diagnosis']\n","        y_test = df_pls.iloc[test]['diagnosis']\n","        \n","        for nd in range(1,maxD):\n","            pls = PLSRegression(n_components=nd)\n","            pls.fit(X_train,y_train)\n","            y_pred = pls.predict(X_test)\n","            rmse = mean_squared_error(y_test, y_pred, squared=False)\n","            temp_dict['nFolds'].append(f)\n","            temp_dict['nDim'].append(nd)\n","            temp_dict['rmse'].append(rmse)\n","\n","pls_results = pd.DataFrame(temp_dict)\n","sns.lineplot(data=pls_results, x='nDim', y='rmse', hue='nFolds')"]},{"cell_type":"markdown","metadata":{},"source":["Ok so about 9 dimensions produces the optimum result, any more than this and it doesn't improve. NB because this is just for evaluation accuracy and not classification (yet) I'm not using the F1 score but rather the predicted value which can be loosely interpreted as a probability value of it belonging to class 1 (malignant)."]},{"cell_type":"markdown","metadata":{},"source":["### Fisher's LDA"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T03:20:56.900266Z","iopub.status.busy":"2023-11-07T03:20:56.899525Z","iopub.status.idle":"2023-11-07T03:20:57.803587Z","shell.execute_reply":"2023-11-07T03:20:57.802318Z","shell.execute_reply.started":"2023-11-07T03:20:56.900226Z"},"trusted":true},"outputs":[],"source":["from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","\n","df_lda = df_pls.copy()\n","\n","for f in range(2,MaxFolds+1):\n","    kf = KFold(n_splits=f, shuffle=True, random_state=42)\n","    \n","    ExpVar = []\n","    RMSE = []\n","    \n","    for train, test in kf.split(df_lda):\n","        X_train = df_lda.iloc[train].drop(['id','diagnosis'], axis=1)\n","        X_test = df_lda.iloc[test].drop(['id','diagnosis'], axis=1)\n","        y_train = df_lda.iloc[train]['diagnosis']\n","        y_test = df_lda.iloc[test]['diagnosis']\n","\n","        lda = LinearDiscriminantAnalysis()\n","        lda.fit(X_train,y_train)\n","        y_pred = lda.predict_proba(X_test)[:,1]\n","        \n","        rmse = mean_squared_error(y_test, y_pred, squared=False)\n","        expvar = lda.explained_variance_ratio_\n","        ExpVar.append(expvar)\n","        RMSE.append(rmse)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T03:20:57.812279Z","iopub.status.busy":"2023-11-07T03:20:57.810868Z","iopub.status.idle":"2023-11-07T03:20:58.382479Z","shell.execute_reply":"2023-11-07T03:20:58.381079Z","shell.execute_reply.started":"2023-11-07T03:20:57.812223Z"},"trusted":true},"outputs":[],"source":["fig, ax = plt.subplots(1,2)\n","# Transform full data set and compare\n","y_whole = lda.predict_proba(df_lda.drop(['id','diagnosis'], axis=1))[:,1]\n","sns.histplot(x=y_whole, hue=df['diagnosis'], ax=ax[0])\n","sns.boxplot(x=RMSE, ax=ax[1])\n","ax[0].set_xlabel('Prediction Probability')\n","ax[0].set_title('FDA on whole data set')\n","\n","ax[1].set_xlabel('RMSE')\n","ax[1].set_title('Accuracy of Prediction')"]},{"cell_type":"markdown","metadata":{},"source":["In fact it seems Fischers DA actually performs better the PLS in predicting the values. However, let's see if after hyper-parameter tuning it will be better."]},{"cell_type":"markdown","metadata":{},"source":["# Model Selection\n","## The aim now is to try and test out various ML solutions to produce the best result\n","* Fischer DA\n","* PLS-DA\n","* RandomForest\n","* XGBoost\n","* LightGBM\n","* SVM\n","* Logistic Regression\n","\n","As a reminder I will optimise based on the F1 score, but I will create a dictionary where its optimum parameters, F1 score and columns used are stored. This means I will score the feature importances separately.\n","\n","Additionally I will also try this on the first 10 PC features, this may also improve the speed of the algorithm"]},{"cell_type":"markdown","metadata":{},"source":["## Do a train-test-val split\n","The first step is to keep a validation data set separate for final comparison of all the optimised models. The train-test split will be done with k-folds=5 on the training data so I will not bother doing the train-test-val split. I will just do train-val.\n","\n","NB I will also assess this for the 10 feature engineered PCs from PCA."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T03:20:58.384258Z","iopub.status.busy":"2023-11-07T03:20:58.383907Z","iopub.status.idle":"2023-11-07T03:20:58.417117Z","shell.execute_reply":"2023-11-07T03:20:58.415847Z","shell.execute_reply.started":"2023-11-07T03:20:58.384227Z"},"trusted":true},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","X = df_norm.drop(labels=['id','diagnosis'], axis=1)\n","y = df_norm['diagnosis'].replace(['M','B'],[1,0])\n","\n","#Perform PCA\n","pca = PCA(n_components=10)\n","Xpca = pca.fit_transform(X)\n","Xpca = pd.DataFrame(Xpca, columns=['PC1','PC2','PC3','PC4','PC5','PC6','PC7','PC8','PC9','PC10'])\n","\n","#Train validation splits\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n","Xpca_train, Xpca_val, ypca_train, ypca_val = train_test_split(Xpca, y, test_size=0.2, random_state=42)\n","\n","print(\"Train size: \",X_train.shape)\n","print(\"Validation size: \",X_val.shape)"]},{"cell_type":"markdown","metadata":{},"source":["## Fischer DA"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T03:20:58.419445Z","iopub.status.busy":"2023-11-07T03:20:58.418886Z","iopub.status.idle":"2023-11-07T03:21:04.699316Z","shell.execute_reply":"2023-11-07T03:21:04.697982Z","shell.execute_reply.started":"2023-11-07T03:20:58.419403Z"},"trusted":true},"outputs":[],"source":["# From direct features\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import classification_report\n","\n","#Set up a dictionary to store information about models\n","Model_dict = {\n","    'Model': [],\n","    'PCA': [],\n","    'F1_Score': [],\n","    'Parameters': [],\n","    'Time': []\n","}\n","\n","Param_Grid = {'solver': ['svd', 'lsqr', 'eigen'],\n","              'shrinkage': np.arange(0,1,0.1),\n","              'tol': np.arange(0,0.001,0.0001)\n","             }\n","\n","lda = LinearDiscriminantAnalysis()\n","\n","Grid_Search = GridSearchCV(\n","    estimator = lda,\n","    param_grid = Param_Grid,\n","    n_jobs = -1,\n","    cv = 5,\n","    scoring = 'f1',\n","    verbose = 1\n",")\n","Grid_Search.fit(X_train,y_train)\n","Best_Parameters = Grid_Search.best_estimator_.get_params()\n","LDA_Params = Best_Parameters\n","print (f'Best score: {np.round(Grid_Search.best_score_,4)}')\n","\n","for Param_Name in sorted(Param_Grid.keys()):\n","    print ('\\t%s: %r' %(Param_Name, Best_Parameters[Param_Name]))\n","\n","y_pred = Grid_Search.predict(X_val)\n","print (classification_report(y_val, y_pred))\n","\n","#Add in Fischer/LinearDiscriminant model\n","Model_dict['Model'].append('LinearDiscriminantAnalysis')\n","Model_dict['PCA'].append('n')\n","Model_dict['F1_Score'].append(Grid_Search.best_score_)\n","Model_dict['Parameters'].append(Best_Parameters)\n","Model_dict['Time'].append(Grid_Search.refit_time_)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T03:21:04.702720Z","iopub.status.busy":"2023-11-07T03:21:04.701864Z","iopub.status.idle":"2023-11-07T03:21:08.396453Z","shell.execute_reply":"2023-11-07T03:21:08.395183Z","shell.execute_reply.started":"2023-11-07T03:21:04.702659Z"},"trusted":true},"outputs":[],"source":["#Engineered Features\n","lda = LinearDiscriminantAnalysis()\n","\n","Grid_Search = GridSearchCV(\n","    estimator = lda,\n","    param_grid = Param_Grid,\n","    n_jobs = -1,\n","    cv = 5,\n","    scoring = 'f1',\n","    verbose = 1\n",")\n","Grid_Search.fit(Xpca_train,ypca_train)\n","Best_Parameters = Grid_Search.best_estimator_.get_params()\n","LDApca_Params = Best_Parameters\n","print (f'Best score: {np.round(Grid_Search.best_score_,4)}')\n","\n","for Param_Name in sorted(Param_Grid.keys()):\n","    print ('\\t%s: %r' %(Param_Name, Best_Parameters[Param_Name]))\n","\n","y_pred = Grid_Search.predict(Xpca_val)\n","print (classification_report(y_val, y_pred))\n","\n","#Add in Fischer/LinearDiscriminant model\n","Model_dict['Model'].append('LinearDiscriminantAnalysis')\n","Model_dict['PCA'].append('y')\n","Model_dict['F1_Score'].append(Grid_Search.best_score_)\n","Model_dict['Parameters'].append(Best_Parameters)\n","Model_dict['Time'].append(Grid_Search.refit_time_)"]},{"cell_type":"markdown","metadata":{},"source":["## PLS\n","For PLS-DA there isn't a plug and play version that works with an F1 score calculation in grid search so I've written it myself\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T03:21:08.398366Z","iopub.status.busy":"2023-11-07T03:21:08.397979Z","iopub.status.idle":"2023-11-07T03:22:04.636485Z","shell.execute_reply":"2023-11-07T03:22:04.635119Z","shell.execute_reply.started":"2023-11-07T03:21:08.398327Z"},"trusted":true},"outputs":[],"source":["#Will have to write my own grid search for PLS because the DA part isn't accomodated in the pipeline\n","#No feature engineering\n","scores_dict = {\n","    'n_components': [],\n","    'scale': [],\n","    'max_iter': [],\n","    'DA_Threshold': [],\n","    'F1_score': []\n","}\n","Components = np.arange(1,15)\n","Scales = [True, False]\n","max_iter = [5, 10, 20]\n","Thresh = np.arange(0.25,0.75,0.05)\n","for nd in Components:\n","    for s in Scales:\n","        for mi in max_iter:\n","            pls = PLSRegression(n_components=nd, scale = s, max_iter=mi)\n","            kf = KFold(n_splits=5)\n","            f1s = []\n","            for th in Thresh:\n","                for train, test in kf.split(Xpca_train):\n","                    #Get the train-test splits per fold\n","                    X_tr = X_train.iloc[train]\n","                    X_te = X_train.iloc[test]\n","                    y_tr = y_train.iloc[train]\n","                    y_te = y_train.iloc[test]\n","\n","                    pls.fit(X_tr,y_tr)\n","                    y_pred = pls.predict(X_te)\n","                    y_final = y_pred >= th\n","                    y_final = y_final*1\n","                    f1s.append(f1_score(y_te,y_final))\n","                f1m = np.mean(f1s)\n","                scores_dict['n_components'].append(nd)\n","                scores_dict['scale'].append(s)\n","                scores_dict['max_iter'].append(mi)\n","                scores_dict['DA_Threshold'].append(th)\n","                scores_dict['F1_score'].append(f1m)\n","                    \n","pls_score_df = pd.DataFrame(scores_dict)\n","max_idx = pls_score_df['F1_score'].argmax()\n","display(pls_score_df.iloc[max_idx])\n","\n","p = {'n_components': pls_score_df.iloc[max_idx]['n_components'],\n","    'scale': pls_score_df.iloc[max_idx]['scale'],\n","    'max_iter': pls_score_df.iloc[max_idx]['max_iter'],\n","    'DA_Threshold': pls_score_df.iloc[max_idx]['DA_Threshold']\n","    }\n","PLS_DA_Params = p\n","\n","#Add in PLS model\n","Model_dict['Model'].append('PLS-DA')\n","Model_dict['PCA'].append('n')\n","Model_dict['F1_Score'].append(pls_score_df.iloc[max_idx]['F1_score'])\n","Model_dict['Parameters'].append(p)\n","\n","#Calculate the time to fit the whole data\n","import time\n","start = time.time()\n","pls = PLSRegression(n_components=p['n_components'], scale = p['scale'], max_iter=p['max_iter'])\n","pls.fit(X_train,y_train)\n","y_final = y_pred >= p['DA_Threshold']\n","y_final = y_final*1\n","end = time.time()\n","\n","Model_dict['Time'].append(end-start)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T03:22:04.639705Z","iopub.status.busy":"2023-11-07T03:22:04.638895Z","iopub.status.idle":"2023-11-07T03:22:22.186887Z","shell.execute_reply":"2023-11-07T03:22:22.185499Z","shell.execute_reply.started":"2023-11-07T03:22:04.639645Z"},"trusted":true},"outputs":[],"source":["#Feature Engineered\n","scores_dict = {\n","    'n_components': [],\n","    'scale': [],\n","    'max_iter': [],\n","    'DA_Threshold': [],\n","    'F1_score': []\n","}\n","Components = np.arange(1,10)\n","Scales = [True, False]\n","max_iter = [5, 10, 20]\n","Thresh = np.arange(0.25,0.75,0.05)\n","for nd in Components:\n","    for s in Scales:\n","        for mi in max_iter:\n","            pls = PLSRegression(n_components=nd, scale = s, max_iter=mi)\n","            kf = KFold(n_splits=5)\n","            f1s = []\n","            for th in Thresh:\n","                for train, test in kf.split(Xpca_train):\n","                    #Get the train-test splits per fold\n","                    X_tr = Xpca_train.iloc[train]\n","                    X_te = Xpca_train.iloc[test]\n","                    y_tr = ypca_train.iloc[train]\n","                    y_te = ypca_train.iloc[test]\n","\n","                    pls.fit(X_tr,y_tr)\n","                    y_pred = pls.predict(X_te)\n","                    y_final = y_pred >= th\n","                    y_final = y_final*1\n","                    f1s.append(f1_score(y_te,y_final))\n","                f1m = np.mean(f1s)\n","                scores_dict['n_components'].append(nd)\n","                scores_dict['scale'].append(s)\n","                scores_dict['max_iter'].append(mi)\n","                scores_dict['DA_Threshold'].append(th)\n","                scores_dict['F1_score'].append(f1m)\n","                    \n","pls_score_df = pd.DataFrame(scores_dict)\n","max_idx = pls_score_df['F1_score'].argmax()\n","display(pls_score_df.iloc[max_idx])\n","\n","p_pca = {'n_components': pls_score_df.iloc[max_idx]['n_components'],\n","    'scale': pls_score_df.iloc[max_idx]['scale'],\n","    'max_iter': pls_score_df.iloc[max_idx]['max_iter'],\n","    'DA_Threshold': pls_score_df.iloc[max_idx]['DA_Threshold']\n","    }\n","PLS_DApca_Params = p_pca\n","\n","#Add in PLS model\n","Model_dict['Model'].append('PLS-DA')\n","Model_dict['PCA'].append('y')\n","Model_dict['F1_Score'].append(pls_score_df.iloc[max_idx]['F1_score'])\n","Model_dict['Parameters'].append(p)\n","\n","#Calculate the time to fit the whole data\n","import time\n","start = time.time()\n","pls = PLSRegression(n_components=p['n_components'], scale = p['scale'], max_iter=p['max_iter'])\n","pls.fit(X_train,y_train)\n","y_final = y_pred >= p['DA_Threshold']\n","y_final = y_final*1\n","end = time.time()\n","\n","Model_dict['Time'].append(end-start)"]},{"cell_type":"markdown","metadata":{},"source":["### Random Forest Classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T03:22:22.189748Z","iopub.status.busy":"2023-11-07T03:22:22.189002Z","iopub.status.idle":"2023-11-07T03:22:28.267405Z","shell.execute_reply":"2023-11-07T03:22:28.265612Z","shell.execute_reply.started":"2023-11-07T03:22:22.189680Z"},"trusted":true},"outputs":[],"source":["from sklearn.ensemble import RandomForestClassifier\n","#No Feature Engineering    \n","Param_Grid = {'n_estimators': [10, 20, 30],\n","              'criterion': ['gini', 'entropy', 'log_loss'],\n","              'max_depth': [40, 50, 75],\n","              'min_samples_leaf': [1],\n","              'max_features' : [None]\n","             }\n","\n","rfc = RandomForestClassifier()\n","\n","Grid_Search = GridSearchCV(\n","    estimator = rfc,\n","    param_grid = Param_Grid,\n","    n_jobs = -1,\n","    cv = 5,\n","    scoring = 'f1',\n","    verbose = 1\n",")\n","Grid_Search.fit(X_train,y_train)\n","Best_Parameters = Grid_Search.best_estimator_.get_params()\n","RFC_Params = Best_Parameters\n","print (f'Best score: {np.round(Grid_Search.best_score_,4)}')\n","for Param_Name in sorted(Param_Grid.keys()):\n","    print ('\\t%s: %r' %(Param_Name, Best_Parameters[Param_Name]))\n","\n","y_pred = Grid_Search.predict(X_val)\n","print (classification_report(y_val, y_pred))\n","\n","#Add in Random Forest model\n","Model_dict['Model'].append('Random Forest Classifier')\n","Model_dict['PCA'].append('n')\n","Model_dict['F1_Score'].append(Grid_Search.best_score_)\n","Model_dict['Parameters'].append(Best_Parameters)\n","Model_dict['Time'].append(Grid_Search.refit_time_)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T03:22:28.270155Z","iopub.status.busy":"2023-11-07T03:22:28.269689Z","iopub.status.idle":"2023-11-07T03:22:31.828145Z","shell.execute_reply":"2023-11-07T03:22:31.826937Z","shell.execute_reply.started":"2023-11-07T03:22:28.270115Z"},"trusted":true},"outputs":[],"source":["#Feature Engineering    \n","rfc = RandomForestClassifier()\n","\n","Grid_Search = GridSearchCV(\n","    estimator = rfc,\n","    param_grid = Param_Grid,\n","    n_jobs = -1,\n","    cv = 5,\n","    scoring = 'f1',\n","    verbose = 1\n",")\n","Grid_Search.fit(Xpca_train,ypca_train)\n","Best_Parameters = Grid_Search.best_estimator_.get_params()\n","RFCpca_Params = Best_Parameters\n","print (f'Best score: {np.round(Grid_Search.best_score_,4)}')\n","\n","for Param_Name in sorted(Param_Grid.keys()):\n","    print ('\\t%s: %r' %(Param_Name, Best_Parameters[Param_Name]))\n","\n","y_pred = Grid_Search.predict(Xpca_val)\n","print (classification_report(ypca_val, y_pred))\n","\n","#Add in Random Forest model\n","Model_dict['Model'].append('Random Forest Classifier')\n","Model_dict['PCA'].append('y')\n","Model_dict['F1_Score'].append(Grid_Search.best_score_)\n","Model_dict['Parameters'].append(Best_Parameters)\n","Model_dict['Time'].append(Grid_Search.refit_time_)"]},{"cell_type":"markdown","metadata":{},"source":["### XGBoost"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T03:22:31.829915Z","iopub.status.busy":"2023-11-07T03:22:31.829508Z","iopub.status.idle":"2023-11-07T03:26:33.295309Z","shell.execute_reply":"2023-11-07T03:26:33.292722Z","shell.execute_reply.started":"2023-11-07T03:22:31.829883Z"},"trusted":true},"outputs":[],"source":["from xgboost import XGBClassifier\n","    \n","Param_Grid = {'max_depth': [2, 5, 10, 25],\n","              'learning_rate': [0.04, 0.05, 0.06, 0.1],\n","              'n_estimators': [400, 500, 600],\n","              'gamma' : [0],\n","              'min_child_weight': [0.01, 0.1, 0.2]\n","             }\n","#No feature engineering\n","xgb = XGBClassifier()\n","\n","Grid_Search = GridSearchCV(\n","    estimator = xgb,\n","    param_grid = Param_Grid,\n","    n_jobs = -1,\n","    cv = 5,\n","    scoring = 'f1',\n","    verbose = 1\n",")\n","Grid_Search.fit(X_train,y_train)\n","Best_Parameters = Grid_Search.best_estimator_.get_params()\n","XGB_Params = Best_Parameters\n","print (f'Best score: {np.round(Grid_Search.best_score_,4)}')\n","for Param_Name in sorted(Param_Grid.keys()):\n","    print ('\\t%s: %r' %(Param_Name, Best_Parameters[Param_Name]))\n","\n","y_pred = Grid_Search.predict(X_val)\n","print (classification_report(y_val, y_pred))\n","#Add in XGB model\n","Model_dict['Model'].append('XGBoost Classifier')\n","Model_dict['PCA'].append('n')\n","Model_dict['F1_Score'].append(Grid_Search.best_score_)\n","Model_dict['Parameters'].append(Best_Parameters)\n","Model_dict['Time'].append(Grid_Search.refit_time_)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T03:26:33.297250Z","iopub.status.busy":"2023-11-07T03:26:33.296828Z"},"trusted":true},"outputs":[],"source":["xgb = XGBClassifier()\n","\n","Grid_Search = GridSearchCV(\n","    estimator = xgb,\n","    param_grid = Param_Grid,\n","    n_jobs = -1,\n","    cv = 5,\n","    scoring = 'f1',\n","    verbose = 1\n",")\n","Grid_Search.fit(Xpca_train,ypca_train)\n","Best_Parameters = Grid_Search.best_estimator_.get_params()\n","XGBpca_Params = Best_Parameters\n","print (f'Best score: {np.round(Grid_Search.best_score_,4)}')\n","for Param_Name in sorted(Param_Grid.keys()):\n","    print ('\\t%s: %r' %(Param_Name, Best_Parameters[Param_Name]))\n","\n","y_pred = Grid_Search.predict(Xpca_val)\n","print (classification_report(ypca_val, y_pred))\n","#Add in XGB model\n","Model_dict['Model'].append('XGBoost Classifier')\n","Model_dict['PCA'].append('y')\n","Model_dict['F1_Score'].append(Grid_Search.best_score_)\n","Model_dict['Parameters'].append(Best_Parameters)\n","Model_dict['Time'].append(Grid_Search.refit_time_)"]},{"cell_type":"markdown","metadata":{},"source":["### Light GBM"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.idle":"2023-11-07T03:32:56.779677Z","shell.execute_reply":"2023-11-07T03:32:56.778540Z","shell.execute_reply.started":"2023-11-07T03:28:31.154165Z"},"trusted":true},"outputs":[],"source":["import lightgbm as LGB\n","#from lightgbm import LGBMClassifier\n","    \n","Param_Grid = {'boosting_type': ['gbdt'],\n","              'num_leaves': [10, 20, 31, 40, 50],\n","              'learning_rate': [0.025, 0.05],\n","              'n_estimators': [100, 400, 500, 600],\n","              'min_child_samples': [1, 5, 10]\n","             }                         \n","                                                  \n","lgb = LGB.LGBMClassifier(objective='binary', random_state=42, max_depth=-1)\n","\n","Grid_Search = GridSearchCV(\n","    estimator = lgb,\n","    param_grid = Param_Grid,\n","    n_jobs = -1,\n","    cv = 5,\n","    scoring = 'f1',\n","    verbose = 1\n",")\n","Grid_Search.fit(X_train,y_train)\n","Best_Parameters = Grid_Search.best_estimator_.get_params()\n","LGB_Params = Best_Parameters\n","print (f'Best score: {np.round(Grid_Search.best_score_,4)}')\n","\n","for Param_Name in sorted(Param_Grid.keys()):\n","    print ('\\t%s: %r' %(Param_Name, Best_Parameters[Param_Name]))\n","\n","y_pred = Grid_Search.predict(X_val)\n","print (classification_report(y_val, y_pred))\n","\n","#Add in LGB model`\n","Model_dict['Model'].append('LightGBM Classifier')\n","Model_dict['PCA'].append('n')\n","Model_dict['F1_Score'].append(Grid_Search.best_score_)\n","Model_dict['Parameters'].append(Best_Parameters)\n","Model_dict['Time'].append(Grid_Search.refit_time_)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T03:32:56.781587Z","iopub.status.busy":"2023-11-07T03:32:56.781242Z","iopub.status.idle":"2023-11-07T03:45:04.524974Z","shell.execute_reply":"2023-11-07T03:45:04.523783Z","shell.execute_reply.started":"2023-11-07T03:32:56.781555Z"},"trusted":true},"outputs":[],"source":["lgb = LGB.LGBMClassifier(objective='binary', random_state=42, max_depth=-1)\n","\n","Grid_Search = GridSearchCV(\n","    estimator = lgb,\n","    param_grid = Param_Grid,\n","    n_jobs = -1,\n","    cv = 5,\n","    scoring = 'f1',\n","    verbose = 1\n",")\n","Grid_Search.fit(Xpca_train,ypca_train)\n","Best_Parameters = Grid_Search.best_estimator_.get_params()\n","LGBpca_Params = Best_Parameters\n","print (f'Best score: {np.round(Grid_Search.best_score_,4)}')\n","\n","for Param_Name in sorted(Param_Grid.keys()):\n","    print ('\\t%s: %r' %(Param_Name, Best_Parameters[Param_Name]))\n","\n","y_pred = Grid_Search.predict(Xpca_val)\n","print (classification_report(ypca_val, y_pred))\n","\n","#Add in LGB model`\n","Model_dict['Model'].append('LightGBM Classifier')\n","Model_dict['PCA'].append('y')\n","Model_dict['F1_Score'].append(Grid_Search.best_score_)\n","Model_dict['Parameters'].append(Best_Parameters)\n","Model_dict['Time'].append(Grid_Search.refit_time_)"]},{"cell_type":"markdown","metadata":{},"source":["### Support Vector Machine"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T03:45:04.527369Z","iopub.status.busy":"2023-11-07T03:45:04.526681Z","iopub.status.idle":"2023-11-07T03:45:04.808010Z","shell.execute_reply":"2023-11-07T03:45:04.806651Z","shell.execute_reply.started":"2023-11-07T03:45:04.527323Z"},"trusted":true},"outputs":[],"source":["from sklearn.svm import SVC\n","Param_Grid = {'C': [0.5, 0.75, 1, 1.25, 2],\n","              'kernel': ['rbf'],\n","              'gamma': ['scale']\n","             }\n","\n","svc = SVC(random_state=42)\n","\n","Grid_Search = GridSearchCV(\n","    estimator = svc,\n","    param_grid = Param_Grid,\n","    n_jobs = -1,\n","    cv = 5,\n","    scoring = 'f1',\n","    verbose = 1\n",")\n","Grid_Search.fit(X_train,y_train)\n","Best_Parameters = Grid_Search.best_estimator_.get_params()\n","SVC_Params = Best_Parameters\n","\n","print (f'Best score: {np.round(Grid_Search.best_score_,4)}')\n","for Param_Name in sorted(Param_Grid.keys()):\n","    print ('\\t%s: %r' %(Param_Name, Best_Parameters[Param_Name]))\n","\n","y_pred = Grid_Search.predict(X_val)\n","print (classification_report(y_val, y_pred))\n","\n","#Add in SVC model\n","Model_dict['Model'].append('Support Vector Classifier')\n","Model_dict['PCA'].append('n')\n","Model_dict['F1_Score'].append(Grid_Search.best_score_)\n","Model_dict['Parameters'].append(Best_Parameters)\n","Model_dict['Time'].append(Grid_Search.refit_time_)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T03:45:04.810470Z","iopub.status.busy":"2023-11-07T03:45:04.810001Z","iopub.status.idle":"2023-11-07T03:45:05.066801Z","shell.execute_reply":"2023-11-07T03:45:05.065552Z","shell.execute_reply.started":"2023-11-07T03:45:04.810426Z"},"trusted":true},"outputs":[],"source":["svc = SVC(random_state=42)\n","\n","Grid_Search = GridSearchCV(\n","    estimator = svc,\n","    param_grid = Param_Grid,\n","    n_jobs = -1,\n","    cv = 5,\n","    scoring = 'f1',\n","    verbose = 1\n",")\n","Grid_Search.fit(Xpca_train,ypca_train)\n","Best_Parameters = Grid_Search.best_estimator_.get_params()\n","SVCpca_Params = Best_Parameters\n","\n","print (f'Best score: {np.round(Grid_Search.best_score_,4)}')\n","for Param_Name in sorted(Param_Grid.keys()):\n","    print ('\\t%s: %r' %(Param_Name, Best_Parameters[Param_Name]))\n","\n","y_pred = Grid_Search.predict(Xpca_val)\n","print (classification_report(ypca_val, y_pred))\n","\n","#Add in SVC model\n","Model_dict['Model'].append('Support Vector Classifier')\n","Model_dict['PCA'].append('y')\n","Model_dict['F1_Score'].append(Grid_Search.best_score_)\n","Model_dict['Parameters'].append(Best_Parameters)\n","Model_dict['Time'].append(Grid_Search.refit_time_)"]},{"cell_type":"markdown","metadata":{},"source":["### Logistic Regressor"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T03:45:05.068549Z","iopub.status.busy":"2023-11-07T03:45:05.068201Z","iopub.status.idle":"2023-11-07T03:45:09.979613Z","shell.execute_reply":"2023-11-07T03:45:09.978437Z","shell.execute_reply.started":"2023-11-07T03:45:05.068517Z"},"trusted":true},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n","Param_Grid = {'penalty': ['l1', 'l2', 'elasticnet', None],\n","              'C': [0.0001, 0.001, 0.005],\n","              'solver': ['lbfgs', 'liblinear'],\n","              'max_iter': [25, 50, 100, 150]\n","             }\n","\n","lrc = LogisticRegression(random_state=42, multi_class='ovr')\n","\n","Grid_Search = GridSearchCV(\n","    estimator = lrc,\n","    param_grid = Param_Grid,\n","    n_jobs = -1,\n","    cv = 5,\n","    scoring = 'f1',\n","    verbose = 1\n",")\n","    \n","Grid_Search.fit(X_train,y_train)\n","Best_Parameters = Grid_Search.best_estimator_.get_params()\n","LR_Params = Best_Parameters\n","\n","print (f'Best score: {np.round(Grid_Search.best_score_,4)}')\n","for Param_Name in sorted(Param_Grid.keys()):\n","    print ('\\t%s: %r' %(Param_Name, Best_Parameters[Param_Name]))\n","\n","y_pred = Grid_Search.predict(X_val)\n","print (classification_report(y_val, y_pred))\n","\n","#Add in Logistic regression model\n","Model_dict['Model'].append('Logistic Regression Classifier')\n","Model_dict['PCA'].append('n')\n","Model_dict['F1_Score'].append(Grid_Search.best_score_)\n","Model_dict['Parameters'].append(Best_Parameters)\n","Model_dict['Time'].append(Grid_Search.refit_time_)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T03:45:09.982214Z","iopub.status.busy":"2023-11-07T03:45:09.981522Z","iopub.status.idle":"2023-11-07T03:45:12.632724Z","shell.execute_reply":"2023-11-07T03:45:12.631553Z","shell.execute_reply.started":"2023-11-07T03:45:09.982170Z"},"trusted":true},"outputs":[],"source":["lrc = LogisticRegression(random_state=42, multi_class='ovr')\n","\n","Grid_Search = GridSearchCV(\n","    estimator = lrc,\n","    param_grid = Param_Grid,\n","    n_jobs = -1,\n","    cv = 5,\n","    scoring = 'f1',\n","    verbose = 1\n",")\n","    \n","Grid_Search.fit(Xpca_train,ypca_train)\n","Best_Parameters = Grid_Search.best_estimator_.get_params()\n","LRpca_Params = Best_Parameters\n","\n","print ('Best score: %0.3f', Grid_Search.best_score_)\n","print ('Best parameters set:', Best_Parameters )\n","for Param_Name in sorted(Param_Grid.keys()):\n","    print ('\\t%s: %r' %(Param_Name, Best_Parameters[Param_Name]))\n","\n","y_pred = Grid_Search.predict(Xpca_val)\n","print (classification_report(ypca_val, y_pred))\n","\n","#Add in Logistic regression model\n","Model_dict['Model'].append('Logistic Regression Classifier')\n","Model_dict['PCA'].append('y')\n","Model_dict['F1_Score'].append(Grid_Search.best_score_)\n","Model_dict['Parameters'].append(Best_Parameters)\n","Model_dict['Time'].append(Grid_Search.refit_time_)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T03:45:12.634532Z","iopub.status.busy":"2023-11-07T03:45:12.634208Z","iopub.status.idle":"2023-11-07T03:45:16.464684Z","shell.execute_reply":"2023-11-07T03:45:16.463193Z","shell.execute_reply.started":"2023-11-07T03:45:12.634505Z"},"trusted":true},"outputs":[],"source":["# Evaluate the models on the validation set\n","Model_Val={'Model':[],'F1_Score':[]}\n","#LDA\n","lda = LinearDiscriminantAnalysis(**LDA_Params)\n","lda.fit(X_train, y_train)\n","y_pred = lda.predict(X_val)\n","f1 = f1_score(y_val,y_pred)\n","Model_Val['Model'].append('LinearDiscriminantAnalysis')\n","Model_Val['F1_Score'].append(f1)\n","\n","lda = LinearDiscriminantAnalysis(**LDApca_Params)\n","lda.fit(Xpca_train, ypca_train)\n","y_pred = lda.predict(Xpca_val)\n","f1 = f1_score(y_val,y_pred)\n","Model_Val['Model'].append('LinearDiscriminantAnalysis_PCA')\n","Model_Val['F1_Score'].append(f1)\n","\n","#PLS-DA\n","pls = PLSRegression(n_components=PLS_DA_Params['n_components'], \n","                    scale = PLS_DA_Params['scale'], \n","                    max_iter=PLS_DA_Params['max_iter'])\n","pls.fit(X_train,y_train)\n","y_pred = pls.predict(X_val)\n","y_final = y_pred >= th\n","y_final = y_final*1\n","f1 = f1_score(y_val,y_final)\n","Model_Val['Model'].append('PLS-DA')\n","Model_Val['F1_Score'].append(f1)\n","\n","pls = PLSRegression(n_components=PLS_DApca_Params['n_components'], \n","                    scale = PLS_DApca_Params['scale'], \n","                    max_iter=PLS_DApca_Params['max_iter'])\n","pls.fit(Xpca_train,ypca_train)\n","y_pred = pls.predict(Xpca_val)\n","y_final = y_pred >= th\n","y_final = y_final*1\n","f1 = f1_score(y_val,y_final)\n","Model_Val['Model'].append('PLS-DA_PCA')\n","Model_Val['F1_Score'].append(f1)\n","\n","\n","#Random Forest Classifier\n","rfc = RandomForestClassifier(**RFC_Params)\n","rfc.fit(X_train, y_train)\n","y_pred = rfc.predict(X_val)\n","f1 = f1_score(y_val,y_pred)\n","Model_Val['Model'].append('Random Forest')\n","Model_Val['F1_Score'].append(f1)\n","\n","rfc = RandomForestClassifier(**RFCpca_Params)\n","rfc.fit(Xpca_train, ypca_train)\n","y_pred = rfc.predict(Xpca_val)\n","f1 = f1_score(y_val,y_pred)\n","Model_Val['Model'].append('Random Forest_PCA')\n","Model_Val['F1_Score'].append(f1)\n","\n","#XGBoost\n","xgb = XGBClassifier(**XGB_Params)\n","xgb.fit(X_train,y_train)\n","y_pred = xgb.predict(X_val)\n","f1 = f1_score(y_val,y_pred)\n","Model_Val['Model'].append('XGBoost')\n","Model_Val['F1_Score'].append(f1)\n","\n","xgb = XGBClassifier(**XGBpca_Params)\n","xgb.fit(Xpca_train,ypca_train)\n","y_pred = xgb.predict(Xpca_val)\n","f1 = f1_score(y_val,y_pred)\n","Model_Val['Model'].append('XGBoost_PCA')\n","Model_Val['F1_Score'].append(f1)\n","\n","#LightGB\n","lgb = LGB.LGBMClassifier(**LGB_Params)\n","lgb.fit(X_train,y_train)\n","y_pred = lgb.predict(X_val)\n","f1 = f1_score(y_val,y_pred)\n","Model_Val['Model'].append('LGBM')\n","Model_Val['F1_Score'].append(f1)\n","\n","lgb = LGB.LGBMClassifier(**LGBpca_Params)\n","lgb.fit(Xpca_train,ypca_train)\n","y_pred = lgb.predict(Xpca_val)\n","f1 = f1_score(y_val,y_pred)\n","Model_Val['Model'].append('LGBM_PCA')\n","Model_Val['F1_Score'].append(f1)\n","\n","#Support Vector Classifier\n","svc = SVC(**SVC_Params)\n","svc.fit(X_train,y_train)\n","y_pred = svc.predict(X_val)\n","f1 = f1_score(y_val,y_pred)\n","Model_Val['Model'].append('SVC')\n","Model_Val['F1_Score'].append(f1)\n","\n","svc = SVC(**SVCpca_Params)\n","svc.fit(Xpca_train,ypca_train)\n","y_pred = svc.predict(Xpca_val)\n","f1 = f1_score(y_val,y_pred)\n","Model_Val['Model'].append('SVC_PCA')\n","Model_Val['F1_Score'].append(f1)\n","\n","#Logistic Regression Classifier\n","lrc = LogisticRegression(**LR_Params)\n","lrc.fit(X_train,y_train)\n","y_pred = lrc.predict(X_val)\n","f1 = f1_score(y_val,y_pred)\n","Model_Val['Model'].append('LogRegressor')\n","Model_Val['F1_Score'].append(f1)\n","\n","lrc = LogisticRegression(**LRpca_Params)\n","lrc.fit(Xpca_train,ypca_train)\n","y_pred = lrc.predict(Xpca_val)\n","f1 = f1_score(y_val,y_pred)\n","Model_Val['Model'].append('LogRegressor_PCA')\n","Model_Val['F1_Score'].append(f1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T03:45:16.467302Z","iopub.status.busy":"2023-11-07T03:45:16.466827Z","iopub.status.idle":"2023-11-07T03:45:17.350064Z","shell.execute_reply":"2023-11-07T03:45:17.348772Z","shell.execute_reply.started":"2023-11-07T03:45:16.467258Z"},"trusted":true},"outputs":[],"source":["# Now plot the results\n","fig, ax = plt.subplots(2,1, figsize=(20,10))\n","sns.barplot(data = Model_dict, x = 'Model', y = 'F1_Score', hue='PCA', ax=ax[0])\n","ax[0].set_ylim(0.8,1)\n","ax[0].set_title('On training data')\n","sns.barplot(data = Model_Val, x = 'Model',y = 'F1_Score', ax=ax[1])\n","ax[1].set_ylim(0.8,1)\n","ax[1].set_title('On validation data')\n","\n","display(pd.DataFrame(Model_Val))"]},{"cell_type":"markdown","metadata":{},"source":["Our two best models are the Random Forest classifier on the PCA data, or the two Logistic regression models. With the LogRegressor values having an F1 score of almost 0.99. Therefore the ultimate question is which one can be implemented faster. I.e. I will now look at the speed of processing"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T03:45:17.352049Z","iopub.status.busy":"2023-11-07T03:45:17.351593Z","iopub.status.idle":"2023-11-07T03:45:17.376790Z","shell.execute_reply":"2023-11-07T03:45:17.375644Z","shell.execute_reply.started":"2023-11-07T03:45:17.352009Z"},"trusted":true},"outputs":[],"source":["display(pd.DataFrame(Model_dict))"]},{"cell_type":"markdown","metadata":{},"source":["Ok by far the fastest is the logistic regression classifier after PCA. With a speed of 7 thousandths of a second so I will pick this to investigate."]},{"cell_type":"markdown","metadata":{},"source":["# Feature selection\n","The logistic regression classifier works best. However, I also want to see which are the required features and which ones just add noise to the system."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T03:45:17.379238Z","iopub.status.busy":"2023-11-07T03:45:17.378786Z","iopub.status.idle":"2023-11-07T03:45:17.687921Z","shell.execute_reply":"2023-11-07T03:45:17.686726Z","shell.execute_reply.started":"2023-11-07T03:45:17.379196Z"},"trusted":true},"outputs":[],"source":["np.random.seed(42)\n","Xpca_train['random_feature'] = np.random.rand(Xpca_train.shape[0])\n","lrc = LogisticRegression(**LR_Params)\n","lrc.fit(Xpca_train,ypca_train)\n","coefficients = lrc.coef_[0]\n","\n","feature_importance = pd.DataFrame({'Feature': Xpca_train.columns, 'Importance': np.abs(coefficients)})\n","feature_importance = feature_importance.sort_values('Importance', ascending=True)\n","feature_importance.plot(x='Feature', y='Importance', kind='barh', figsize=(10, 6))"]},{"cell_type":"markdown","metadata":{},"source":["It appears that all features apart from `PC6` is important, but I estimate that the coding for this will slow it down significantly. So I will keep `PC6` within the model."]},{"cell_type":"markdown","metadata":{},"source":["Now the aim is to sum up all the steps into one single step\n","\n","# Final Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T03:45:17.690057Z","iopub.status.busy":"2023-11-07T03:45:17.689582Z","iopub.status.idle":"2023-11-07T03:45:17.792155Z","shell.execute_reply":"2023-11-07T03:45:17.789490Z","shell.execute_reply.started":"2023-11-07T03:45:17.690014Z"},"trusted":true},"outputs":[],"source":["from sklearn.metrics import PrecisionRecallDisplay\n","from sklearn.metrics import precision_recall_curve\n","\n","# Step 1 extract the key fields only\n","X = df.drop(labels=['id','diagnosis'], axis=1)\n","y = df['diagnosis']\n","\n","# Step 2 apply a min-max scaler, and transform y\n","mms = MinMaxScaler()\n","mms.fit(X)\n","X_norm = mms.transform(X)\n","\n","y = y.replace(['M','B'],[1,0])\n","\n","#Step 3 train-test split\n","X_train, X_val, y_train, y_val = train_test_split(X_norm, y, test_size=0.2, random_state=42)\n","\n","#Step 4 perform PCA for 10 dimensions\n","pca = PCA(n_components=10)\n","Xtr = pca.fit_transform(X_train)\n","Xte = pca.transform(X_val)\n","\n","ColNames = ['PC1','PC2','PC3','PC4','PC5','PC6','PC7','PC8','PC9','PC10']\n","X_train = pd.DataFrame(Xtr, columns=ColNames)\n","X_val = pd.DataFrame(Xte, columns=ColNames )\n","\n","# Step 5 train the model\n","lrc = LogisticRegression(**LRpca_Params)\n","lrc.fit(X_train,y_train)\n","y_pred = lrc.predict(X_val)\n","y_probs = lrc.predict_proba(X_val)[:,1]\n","\n","# Step 5 predict and plot a precision recall curve\n","# fig, ax = plt.subplots(1,2, figsize=(10,5))\n","# ax[0] = PrecisionRecallDisplay.from_predictions(y_val, y_pred, name=\"Logistic Regression\")\n","# ax[0].set_title(\"2-class Precision-Recall curve\")\n","precision, recall, thresholds = precision_recall_curve(y_val, y_probs)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T03:45:17.794996Z","iopub.status.busy":"2023-11-07T03:45:17.794635Z","iopub.status.idle":"2023-11-07T03:45:18.193100Z","shell.execute_reply":"2023-11-07T03:45:18.191791Z","shell.execute_reply.started":"2023-11-07T03:45:17.794966Z"},"trusted":true},"outputs":[],"source":["ax = sns.lineplot(x=precision, y=recall)\n","ax.set_xlabel('Precision')\n","ax.set_ylabel('Recall')\n","ax.set_title('Precision Recall Curve for a logistic regression classifier')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T03:45:18.197199Z","iopub.status.busy":"2023-11-07T03:45:18.195877Z","iopub.status.idle":"2023-11-07T03:45:18.202577Z","shell.execute_reply":"2023-11-07T03:45:18.201540Z","shell.execute_reply.started":"2023-11-07T03:45:18.197149Z"},"trusted":true},"outputs":[],"source":["print(LR_Params)"]},{"cell_type":"markdown","metadata":{},"source":["So to conclude the best solution is...\n","1. Normalise the data with a MinMaxScaler\n","2. Apply a 10 dimensional PCA\n","3. A logistic regression classifier, the parameters as above"]},{"cell_type":"markdown","metadata":{},"source":["# Saving outputs\n","What I will need to do....\n","* Using pickle, save a file with\n","    1. the MinMaxScaler\n","    2. the PCA at 10 dimensions\n","    3. the LogisticRegressionClassifier\n","* Write a .py file to train the model\n","\n","## Create train.py (in order to save a pickle using the data.csv file)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T03:45:18.204600Z","iopub.status.busy":"2023-11-07T03:45:18.204220Z","iopub.status.idle":"2023-11-07T03:45:18.465729Z","shell.execute_reply":"2023-11-07T03:45:18.464531Z","shell.execute_reply.started":"2023-11-07T03:45:18.204568Z"},"trusted":true},"outputs":[],"source":["### Libraries Needed \n","import pandas as pd\n","import numpy as np\n","import pickle\n","\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.decomposition import PCA\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import f1_score\n","\n","### Input Variables\n","data_file = \"data.csv\"\n","Model_Params = {\n","    'class_weight': None, \n","    'dual': False, \n","    'fit_intercept': True, \n","    'intercept_scaling': 1,\n","    'max_iter': 200, #Increased from the 15 to stop a warning \n","    'multi_class': 'ovr', \n","    'n_jobs': None, \n","    'penalty': None, \n","    'random_state': 42, \n","    'solver': 'lbfgs', \n","    'tol': 0.0001, \n","    'verbose': 0, \n","    'warm_start': False\n","} #NB because no penalty C and L1_ratio are not needed\n","ModelFileName = 'model.bin'\n","\n","## Load the whole dataset\n","print('Reading in training data...')\n","df = pd.read_csv(data_file)\n","## Split into variables\n","X = df.drop(labels=['id','diagnosis','Unnamed: 32'], axis=1)\n","y = df['diagnosis'].replace(['M','B'],[1,0])\n","\n","print('Scaling data...')\n","## MinMaxScaler\n","mms = MinMaxScaler()\n","mms.fit(X)\n","X_norm = mms.transform(X)\n","\n","## PCA\n","print('Performing dimension reduction...')\n","pca = PCA(n_components=10)\n","Xt = pca.fit_transform(X_norm)\n","\n","## Get some stats about the fit by doing K-Folds\n","print('Evaluating model performance...')\n","kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","f1_scores = np.zeros(5)\n","c = 0\n","for train, test in kf.split(Xt):\n","    X_train = Xt[train,:]\n","    X_test = Xt[test,:]\n","    y_train = y[train]\n","    y_test = y[test]\n","    \n","    lrc = LogisticRegression(**Model_Params)\n","    lrc.fit(X_train,y_train)\n","    y_pred = lrc.predict(X_test)\n","    f1 = f1_score(y_test,y_pred)\n","    f1_scores[c] = f1\n","    result = f'Fold Number: {c+1}, F1 score: {np.round(f1,4)}'\n","    print(result)\n","    c += 1\n","result = f'Mean F1 Score: {np.mean(f1_scores)}'\n","print(result)\n","      \n","## Now train the final model\n","print('Training final model...')\n","lrc = LogisticRegression(**Model_Params)\n","lrc.fit(Xt,y)\n","\n","print('Saving final model...')\n","with open(ModelFileName, 'wb') as f_out: # 'wb' means write-binary\n","    pickle.dump((mms, pca, lrc, Model_Params), f_out)\n","\n","print('Model saved.')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T03:45:18.467721Z","iopub.status.busy":"2023-11-07T03:45:18.467337Z","iopub.status.idle":"2023-11-07T03:45:18.475680Z","shell.execute_reply":"2023-11-07T03:45:18.474604Z","shell.execute_reply.started":"2023-11-07T03:45:18.467673Z"},"trusted":true},"outputs":[],"source":["df.columns"]},{"cell_type":"markdown","metadata":{},"source":["### Run saved model"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T03:45:18.477250Z","iopub.status.busy":"2023-11-07T03:45:18.476944Z","iopub.status.idle":"2023-11-07T03:45:18.486010Z","shell.execute_reply":"2023-11-07T03:45:18.485059Z","shell.execute_reply.started":"2023-11-07T03:45:18.477224Z"},"trusted":true},"outputs":[],"source":["import pickle\n","import pandas as pd\n","\n","with open('model.bin', 'rb') as f_in: # very important to use 'rb' here, it means read-binary \n","    mms, pca, lrc, LRpca_Params = pickle.load(f_in)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T03:45:18.488367Z","iopub.status.busy":"2023-11-07T03:45:18.487380Z","iopub.status.idle":"2023-11-07T03:45:18.497780Z","shell.execute_reply":"2023-11-07T03:45:18.496736Z","shell.execute_reply.started":"2023-11-07T03:45:18.488327Z"},"trusted":true},"outputs":[],"source":["cell = {\n","    'radius_mean': 17.99,\n","    'texture_mean': 10.38,\n","    'perimeter_mean': 122.80, \n","    'area_mean': 1001.0,\n","    'smoothness_mean': 0.11840,\n","    'compactness_mean': 0.27760,\n","    'concavity_mean': 0.30010, \n","    'concave points_mean': 0.14710,\n","    'symmetry_mean': 0.2419,\n","    'fractal_dimension_mean': 0.07871,\n","    'radius_se':  1.095,\n","    'texture_se': 0.9053,\n","    'perimeter_se': 8.589,\n","    'area_se': 153.4,\n","    'smoothness_se': 0.006399,\n","    'compactness_se': 0.04904,\n","    'concavity_se': 0.05373,\n","    'concave points_se': 0.0158,\n","    'symmetry_se':  0.03003,\n","    'fractal_dimension_se': 0.006193,\n","    'radius_worst': 25.38,\n","    'texture_worst': 17.33,\n","    'perimeter_worst': 184.6,\n","    'area_worst': 2019.0,\n","    'smoothness_worst': 0.1622,\n","    'compactness_worst': 0.6656,\n","    'concavity_worst': 0.7119,\n","    'concave points_worst': 0.2654,\n","    'symmetry_worst': 0.4601,\n","    'fractal_dimension_worst': 0.11890\n","}"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T04:26:24.164441Z","iopub.status.busy":"2023-11-07T04:26:24.163997Z","iopub.status.idle":"2023-11-07T04:26:24.177393Z","shell.execute_reply":"2023-11-07T04:26:24.176295Z","shell.execute_reply.started":"2023-11-07T04:26:24.164405Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Cell Diagnosis: Malignant\n","p Malignant: 0.9999999999998272\n"]}],"source":["# Predict function\n","def transform_data(cell):\n","    df_X = pd.DataFrame(cell, index=[0])\n","    iX = mms.transform(df_X)\n","    X = pca.transform(iX)\n","    return X\n","\n","def predict_from_model(X):\n","    y_pred = lrc.predict_proba(X)[0,1]\n","    if y_pred >= 0.5:\n","        Diagnosis = \"Malignant\"\n","    else:\n","        Diagnosis = \"Benign\"\n","    return Diagnosis, y_pred\n","            \n","X = transform_data(cell)\n","Diagnosis, y_pred = predict_from_model(X)\n","text = f\"Cell Diagnosis: {Diagnosis}\\np Malignant: {y_pred}\"\n","print(text)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":4}
